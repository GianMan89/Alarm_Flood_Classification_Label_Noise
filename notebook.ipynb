{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03ec76f3",
   "metadata": {},
   "source": [
    "## Imports and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c502c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from typing import Optional, List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, Flatten, Activation, CenterCrop\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "from classifiers.WDI_1NN import WDI_1NN\n",
    "from classifiers.ACM_SVM import ACM_SVM\n",
    "from classifiers.CASIM import CASIM\n",
    "from classifiers.EAC_1NN import EAC_KNN\n",
    "from classifiers.MBW_LR import MBW_LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e2445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "N_REPEATS = 1  # TODO: set to 10\n",
    "N_SPLITS = 5\n",
    "MAX_DATA_LENGTH = 60\n",
    "PERTURBATION_STEP_SIZE = 0.1   # TODO: set to 0.01\n",
    "PERTURBATION_STEPS = 10    # TODO: set to 100\n",
    "NUM_EPOCHS = 10 # TODO: set to 50\n",
    "\n",
    "SAVE_DIR_TEP = \"results/tep\"\n",
    "SAVE_DIR_FCC = \"results/fcc\"\n",
    "\n",
    "wdi_1nn_params = {\"template_threshold\": 0.5, \"n_neighbors\": 1}\n",
    "acm_svm_params = {None}\n",
    "casim_params={\n",
    "        \"num_features\": 672,\n",
    "        \"n_estimators\": 10,\n",
    "        \"n_jobs_multirocket\": 1,\n",
    "        \"random_state\": RANDOM_SEED,\n",
    "        \"alphas\": np.logspace(-3, 3, 10),\n",
    "    }\n",
    "eac_1nn_params = {\"attenuation_coefficient_per_min\": 0.0667, \"n_neighbors\": 1}\n",
    "mbw_lr_params = {\n",
    "        \"penalty\": None,\n",
    "        \"fit_intercept\": False,\n",
    "        \"solver\": \"lbfgs\",\n",
    "        \"multi_class\": \"ovr\",\n",
    "        \"decision_bounds\": True,\n",
    "        \"confidence_interval\": 1.96,\n",
    "    }\n",
    "\n",
    "CLASSIFIERS = {\n",
    "    \"WDI_1NN\": (WDI_1NN, wdi_1nn_params),\n",
    "    #\"CASIM\": (CASIM, casim_params),\n",
    "    \"EAC_1NN\": (EAC_KNN, eac_1nn_params),\n",
    "    #\"MBW_LR\": (MBW_LR, mbw_lr_params),\n",
    "    #\"ACM_SVM\": (ACM_SVM, acm_svm_params),\n",
    "}\n",
    "\n",
    "CLASSIFIERS_EAC_KNN = {}\n",
    "for i in [1, 5, 10, 25, 50]:\n",
    "    params = {\"attenuation_coefficient_per_min\": 0.0667, \"n_neighbors\": i}\n",
    "    CLASSIFIERS_EAC_KNN[f\"EAC_{i}NN\"] = (EAC_KNN, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec45514",
   "metadata": {},
   "source": [
    "## Load Data and Original Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeea7716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_from_folder(base_path, max_data_length):\n",
    "    \"\"\"\n",
    "    Load time-series CSV files organized in subfolders per class.\n",
    "\n",
    "    Parameters\n",
    "    - base_path: str or Path to the folder containing one subfolder per class.\n",
    "                 Each class subfolder should contain CSV files (all same shape).\n",
    "    - max_data_length: int, maximum length of the time series data (number of timesteps).\n",
    "\n",
    "    Returns\n",
    "    - X: numpy array of shape (n_samples, n_variables, n_timesteps)\n",
    "    - y: numpy array of integer labels (0..n_classes-1)\n",
    "\n",
    "    Prints shapes of X and y.\n",
    "    \"\"\"\n",
    "\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    # Find subfolders (classes) in deterministic order\n",
    "    class_folders = sorted(\n",
    "        [\n",
    "            d for d in os.listdir(base_path)\n",
    "            if os.path.isdir(os.path.join(base_path, d))\n",
    "        ]\n",
    "    )\n",
    "    if not class_folders:\n",
    "        raise ValueError(f\"No class subfolders found in {base_path}\")\n",
    "\n",
    "    # Build list of files to load (with labels) to show a single progress bar\n",
    "    files_to_load = []\n",
    "    for label, class_name in enumerate(class_folders):\n",
    "        class_dir = os.path.join(base_path, class_name)\n",
    "        # Collect CSV files in deterministic order\n",
    "        csv_files = sorted(\n",
    "            [\n",
    "                f for f in os.listdir(class_dir)\n",
    "                if os.path.isfile(os.path.join(class_dir, f)) and f.lower().endswith(\".csv\")\n",
    "            ]\n",
    "        )\n",
    "        if not csv_files:\n",
    "            raise ValueError(f\"No CSV files found in class folder {class_dir}\")\n",
    "\n",
    "        for fname in csv_files:\n",
    "            files_to_load.append((os.path.join(class_dir, fname), label))\n",
    "\n",
    "    # Iterate with tqdm progress bar\n",
    "    for csv_path, label in tqdm(files_to_load, desc=\"Loading dataset\", unit=\"file\"):\n",
    "        df = pd.read_csv(csv_path, index_col=0)\n",
    "        arr = df.values.T  # transpose to match original layout: (variables, timesteps)\n",
    "        data.append(arr)\n",
    "        labels.append(label)\n",
    "\n",
    "    # Ensure all samples have the same shape\n",
    "    shapes = {a.shape for a in data}\n",
    "    if len(shapes) != 1:\n",
    "        raise ValueError(f\"Inconsistent sample shapes found: {shapes}\")\n",
    "\n",
    "    X = np.array(data)\n",
    "    y = np.array(labels)\n",
    "\n",
    "    # Resize data to max_data_length if necessary\n",
    "    if X.shape[2] > max_data_length:\n",
    "        X = X[:, :, :max_data_length]\n",
    "\n",
    "    print(\"Data shape: {}\".format(X.shape))\n",
    "    print(\"Labels shape: {}\".format(y.shape))\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4a916b",
   "metadata": {},
   "source": [
    "## Label Perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1d05b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAELabelPerturber:\n",
    "    \"\"\"\n",
    "    Class to handle label perturbation operations based on autoencoder latent distances.\n",
    "\n",
    "    The autoencoder is a convolutional encoder-decoder:\n",
    "    - Input: (features, time)\n",
    "    - Internally reshaped to (features, time, 1)\n",
    "    - Encoder: Conv2D stack -> latent feature map\n",
    "    - Encoder model outputs flattened latent vector (for Euclidean distances)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 random_state: Optional[int] = None,\n",
    "                 latent_dim: int = 32,\n",
    "                 epochs: int = 10):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        random_state : int or None\n",
    "            Random seed for reproducibility.\n",
    "        latent_dim : int\n",
    "            Number of filters in the final encoder Conv2D layer (depth of latent_map).\n",
    "        epochs : int\n",
    "            Number of epochs for autoencoder training.\n",
    "        \"\"\"\n",
    "        self.random_state = random_state\n",
    "        self.latent_dim = latent_dim\n",
    "        self.epochs = epochs\n",
    "\n",
    "        self.perturbation_map: Optional[List[Dict[str, Any]]] = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "\n",
    "        self.autoencoder: Optional[Model] = None\n",
    "        self.encoder: Optional[Model] = None\n",
    "\n",
    "        # Scaler for normalizing distances to [0, 1]\n",
    "        self.scaler = MinMaxScaler()\n",
    "\n",
    "    def _build_autoencoder(self, input_shape):\n",
    "        \"\"\"\n",
    "        Build CNN autoencoder model for binary time series data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_shape : tuple\n",
    "            Shape of a single sample (features, time).\n",
    "        \"\"\"\n",
    "        height, width = input_shape  # (features, time)\n",
    "\n",
    "        # Add channel dimension for Conv2D: (features, time, 1)\n",
    "        inp = Input(shape=(height, width, 1), name=\"encoder_input\")\n",
    "\n",
    "        # Encoder (all CNN)\n",
    "        x = Conv2D(32, 3, strides=2, padding=\"same\",\n",
    "                   activation=\"relu\", name=\"enc_conv_32\")(inp)\n",
    "        x = Conv2D(64, 3, strides=2, padding=\"same\",\n",
    "                   activation=\"relu\", name=\"enc_conv_64\")(x)\n",
    "        x = Conv2D(128, 3, strides=2, padding=\"same\",\n",
    "                   activation=\"relu\", name=\"enc_conv_128\")(x)\n",
    "        latent_map = Conv2D(self.latent_dim, 3, strides=2, padding=\"same\",\n",
    "                            activation=\"relu\", name=\"latent_map\")(x)\n",
    "\n",
    "        # Decoder (mirror of encoder)\n",
    "        y = Conv2DTranspose(128, 3, strides=2, padding=\"same\",\n",
    "                            activation=\"relu\", name=\"dec_deconv_128\")(latent_map)\n",
    "        y = Conv2DTranspose(64, 3, strides=2, padding=\"same\",\n",
    "                            activation=\"relu\", name=\"dec_deconv_64\")(y)\n",
    "        y = Conv2DTranspose(32, 3, strides=2, padding=\"same\",\n",
    "                            activation=\"relu\", name=\"dec_deconv_32\")(y)\n",
    "        y = Conv2DTranspose(1, 3, strides=2, padding=\"same\",\n",
    "                            name=\"dec_deconv_1\")(y)\n",
    "\n",
    "        # Sigmoid -> probabilities of \"alarm active\"\n",
    "        y = Activation(\"sigmoid\", name=\"recon_prob\")(y)\n",
    "\n",
    "        # Ensure exact spatial size (features, time)\n",
    "        y = CenterCrop(height, width, name=\"center_crop\")(y)\n",
    "\n",
    "        # Autoencoder model: from input to reconstructed probabilities\n",
    "        self.autoencoder = Model(inp, y, name=\"autoencoder\")\n",
    "\n",
    "        # Encoder model: from input to flattened latent vector\n",
    "        latent_flat = Flatten(name=\"latent_flat\")(latent_map)\n",
    "        self.encoder = Model(inp, latent_flat, name=\"encoder\")\n",
    "\n",
    "        # Loss and metric (binary reconstruction)\n",
    "        bce_loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "        bin_acc = tf.keras.metrics.BinaryAccuracy(\n",
    "            threshold=0.5, name=\"bin_acc\"\n",
    "        )\n",
    "\n",
    "        self.autoencoder.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss=bce_loss,\n",
    "            metrics=[bin_acc],\n",
    "        )\n",
    "\n",
    "    def _latent_distance(self, latent1: np.ndarray, latent2: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Euclidean distance between latent representations.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        latent1, latent2 : np.ndarray\n",
    "            Latent-space vectors.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Euclidean distance.\n",
    "        \"\"\"\n",
    "        euclidean_dist = np.linalg.norm(latent1 - latent2)\n",
    "        return euclidean_dist\n",
    "\n",
    "    def fit_labels(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        Fit the perturber by training the autoencoder and creating the perturbation map.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            Input data of shape (n_samples, features, time). Values are expected to be binary (0/1)\n",
    "            or at least bounded in [0, 1].\n",
    "        y : np.ndarray\n",
    "            Array of class labels (one per sample).\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        n_samples = len(X)\n",
    "\n",
    "        # Build autoencoder for this input shape\n",
    "        input_shape = X.shape[1:]  # (features, time)\n",
    "        self._build_autoencoder(input_shape)\n",
    "\n",
    "        # Normalize input data to [0, 1] if possible\n",
    "        X_float = X.astype(\"float32\")\n",
    "        max_val = np.max(X_float)\n",
    "        if max_val > 0.0:\n",
    "            X_normalized = X_float / max_val\n",
    "        else:\n",
    "            X_normalized = X_float\n",
    "\n",
    "        # Add channel dimension: (n_samples, features, time, 1)\n",
    "        X_ae = X_normalized[..., np.newaxis]\n",
    "\n",
    "        # Train autoencoder\n",
    "        print(f\"Training autoencoder for {self.epochs} epochs...\")\n",
    "        if self.random_state is not None:\n",
    "            tf.random.set_seed(self.random_state)\n",
    "            np.random.seed(self.random_state)\n",
    "\n",
    "        history = self.autoencoder.fit(\n",
    "            X_ae, X_ae,\n",
    "            epochs=self.epochs,\n",
    "            batch_size=32,\n",
    "            verbose=1,\n",
    "            validation_split=0.1,\n",
    "        )\n",
    "        print(\"Autoencoder training completed.\")\n",
    "\n",
    "        # Get latent representations (flattened)\n",
    "        latent_representations = self.encoder.predict(X_ae, verbose=0)\n",
    "\n",
    "        # Normalize distances using all pairwise distances\n",
    "        all_distances = []\n",
    "        for i in range(n_samples):\n",
    "            for j in range(i + 1, n_samples):\n",
    "                dist = self._latent_distance(\n",
    "                    latent_representations[i], latent_representations[j]\n",
    "                )\n",
    "                all_distances.append(dist)\n",
    "\n",
    "        all_distances = np.array(all_distances).reshape(-1, 1)\n",
    "        self.scaler.fit(all_distances)\n",
    "\n",
    "        # Build perturbation map: for each sample, find closest sample from a different class\n",
    "        self.perturbation_map = []\n",
    "\n",
    "        for i in tqdm(range(n_samples), desc=\"Building perturbation map\"):\n",
    "            current_latent = latent_representations[i]\n",
    "            current_label = y[i]\n",
    "\n",
    "            min_distance = float(\"inf\")\n",
    "            closest_different_label = None\n",
    "\n",
    "            for j in range(n_samples):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                if y[j] == current_label:\n",
    "                    continue\n",
    "\n",
    "                distance = self._latent_distance(\n",
    "                    current_latent, latent_representations[j]\n",
    "                )\n",
    "                distance_normalized = self.scaler.transform([[distance]])[0][0]\n",
    "\n",
    "                if distance_normalized < min_distance:\n",
    "                    min_distance = distance_normalized\n",
    "                    closest_different_label = y[j]\n",
    "\n",
    "            self.perturbation_map.append(\n",
    "                {\n",
    "                    \"original_label\": current_label,\n",
    "                    \"new_label\": closest_different_label,\n",
    "                    \"distance\": min_distance,\n",
    "                }\n",
    "            )\n",
    "        print(\"Built perturbation map.\")\n",
    "\n",
    "    def perturb_labels(self,\n",
    "                       y: np.ndarray,\n",
    "                       perturbation_level: int,\n",
    "                       perturbation_step_size: float = 0.1) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Perturb labels based on the perturbation map and a given perturbation level.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : np.ndarray\n",
    "            Array of labels (should match the y used in fit_labels).\n",
    "        perturbation_level : int\n",
    "            Integer step level (0 = no perturbation).\n",
    "        perturbation_step_size : float\n",
    "            Fraction of samples to perturb per step (e.g., 0.1 = 10% per level).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Perturbed labels.\n",
    "        \"\"\"\n",
    "        if perturbation_level == 0 or self.perturbation_map is None:\n",
    "            return y.copy()\n",
    "\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "\n",
    "        y_perturbed = y.copy()\n",
    "        num_samples = len(y)\n",
    "        num_perturb = int(num_samples * perturbation_step_size * perturbation_level)\n",
    "\n",
    "        if num_perturb == 0:\n",
    "            return y_perturbed\n",
    "\n",
    "        # Sort samples by distance (ascending) to prioritize closest different-class samples\n",
    "        sorted_indices = sorted(\n",
    "            range(len(self.perturbation_map)),\n",
    "            key=lambda i: self.perturbation_map[i][\"distance\"],\n",
    "        )\n",
    "\n",
    "        # Select the samples with lowest distances for perturbation\n",
    "        perturb_indices = sorted_indices[: min(num_perturb, len(sorted_indices))]\n",
    "\n",
    "        for idx in perturb_indices:\n",
    "            new_label = self.perturbation_map[idx][\"new_label\"]\n",
    "            if new_label is not None:\n",
    "                y_perturbed[idx] = new_label\n",
    "\n",
    "        return y_perturbed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ecdf88",
   "metadata": {},
   "source": [
    "## Robustness Tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a1e706",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustnessTester:\n",
    "    \"\"\"Class to test model robustness against label perturbations\"\"\"\n",
    "    \n",
    "    def __init__(self, classifiers_dict, perturber, n_splits=5, n_repeats=1, perturbation_step_size=0.1, \n",
    "                 random_state=None):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - classifiers_dict: dictionary with classifier info {name: (class, params)}\n",
    "        - perturber: LabelPerturber instance\n",
    "        - n_splits: number of CV folds\n",
    "        - n_repeats: number of CV repeats\n",
    "        - perturbation_step_size: percentage per perturbation step\n",
    "        - random_state: random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.classifiers_dict = classifiers_dict\n",
    "        self.perturber = perturber\n",
    "        self.n_splits = n_splits\n",
    "        self.n_repeats = n_repeats\n",
    "        self.perturbation_step_size = perturbation_step_size\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def _process_confusion_matrices(self, confusion_matrices, classifier_name, y):\n",
    "        \"\"\"Helper function to process confusion matrices into DataFrame\"\"\"\n",
    "        cm_list = []\n",
    "        for step, cm in confusion_matrices.items():\n",
    "            cm_flat = cm.flatten()\n",
    "            cm_dict = {f'cm_{i}_{j}': cm_flat[i * len(np.unique(y)) + j] \n",
    "                      for i in range(len(np.unique(y))) \n",
    "                      for j in range(len(np.unique(y)))}\n",
    "            cm_dict['classifier'] = classifier_name\n",
    "            cm_dict['perturbation_percentage'] = round(self.perturbation_step_size * step * 100)\n",
    "            cm_list.append(cm_dict)\n",
    "        \n",
    "        return pd.DataFrame(cm_list).set_index('perturbation_percentage')\n",
    "    \n",
    "    def _process_perturbation_maps(self, perturbation_maps_per_fold, y_train_folds):\n",
    "        \"\"\"Helper function to process perturbation maps into DataFrame\"\"\"\n",
    "        pm_list = []\n",
    "        unique_labels = np.unique(np.concatenate(y_train_folds))\n",
    "        \n",
    "        for fold, fold_maps in enumerate(perturbation_maps_per_fold):\n",
    "            # Initialize confusion matrix structure for this fold\n",
    "            cm_shape = (len(unique_labels), len(unique_labels))\n",
    "            fold_cm = np.zeros(cm_shape, dtype=int)\n",
    "            \n",
    "            # Aggregate all perturbations for this fold\n",
    "            for perturbation in fold_maps:\n",
    "                original_label = perturbation['original_label']\n",
    "                target_label = perturbation['new_label']\n",
    "                \n",
    "                # Convert labels to indices for confusion matrix\n",
    "                orig_idx = np.where(unique_labels == original_label)[0][0]\n",
    "                target_idx = np.where(unique_labels == target_label)[0][0]\n",
    "                fold_cm[orig_idx, target_idx] += 1\n",
    "            \n",
    "            # Convert to dictionary format\n",
    "            cm_flat = fold_cm.flatten()\n",
    "            cm_dict = {f'cm_{i}_{j}': cm_flat[i * len(unique_labels) + j] \n",
    "                      for i in range(len(unique_labels)) \n",
    "                      for j in range(len(unique_labels))}\n",
    "            cm_dict['fold'] = fold\n",
    "            pm_list.append(cm_dict)\n",
    "        \n",
    "        return pd.DataFrame(pm_list).set_index('fold')\n",
    "        \n",
    "    def test_robustness(self, X, y, perturbation_steps=10):\n",
    "        \"\"\"\n",
    "        Test robustness of all classifiers against label perturbation.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: feature data\n",
    "        - y: labels\n",
    "        - perturbation_steps: maximum perturbation level to test\n",
    "        \n",
    "        Returns:\n",
    "        - results dict with DataFrames for each classifier\n",
    "        - confusion_matrices dict with DataFrames for each classifier\n",
    "        - perturbation_maps DataFrame (shared across all classifiers)\n",
    "        \"\"\"\n",
    "        np.random.seed(self.random_state)\n",
    "        skf = RepeatedStratifiedKFold(n_splits=self.n_splits, n_repeats=self.n_repeats, random_state=self.random_state)\n",
    "        \n",
    "        # Initialize storage for all classifiers\n",
    "        all_results = {name: [] for name in self.classifiers_dict.keys()}\n",
    "        all_confusion_matrices = {name: {step: None for step in range(0, perturbation_steps + 1)} \n",
    "                                 for name in self.classifiers_dict.keys()}\n",
    "        perturbation_maps_per_fold = []\n",
    "        y_train_folds = []\n",
    "        \n",
    "        # Main loop: folds first\n",
    "        for fold, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "            print(f\"Processing Fold {fold}...\")\n",
    "            \n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            y_train_folds.append(y_train)\n",
    "            \n",
    "            # Fit perturber on training data (once per fold)\n",
    "            self.perturber.fit_labels(X_train, y_train)\n",
    "            \n",
    "            # Store perturbation maps for this fold (shared across all classifiers)\n",
    "            fold_perturbation_maps = []\n",
    "            \n",
    "            # Loop over perturbation steps\n",
    "            for step in tqdm(range(0, perturbation_steps + 1), desc=\"Perturbation steps\"):\n",
    "                # Perturb training labels using fitted perturber\n",
    "                y_train_perturbed = self.perturber.perturb_labels(\n",
    "                    y_train, step, self.perturbation_step_size\n",
    "                )\n",
    "                \n",
    "                # Store perturbation map for this step (only once per fold)\n",
    "                if step == 0:\n",
    "                    fold_perturbation_maps = []\n",
    "                fold_perturbation_maps.extend(self.perturber.perturbation_map.copy())\n",
    "                \n",
    "                # Loop over classifiers\n",
    "                for classifier_name in self.classifiers_dict.keys():\n",
    "                    classifier_class, classifier_params = self.classifiers_dict[classifier_name]\n",
    "                    \n",
    "                    # Train model on perturbed data\n",
    "                    model_instance = classifier_class(classifier_params)\n",
    "                    model_instance.fit(X_train, y_train_perturbed)\n",
    "                    \n",
    "                    # Evaluate on unperturbed test data\n",
    "                    y_pred = model_instance.predict(X_test)\n",
    "                    accuracy = accuracy_score(y_test, y_pred)\n",
    "                    \n",
    "                    all_results[classifier_name].append({\n",
    "                        'fold': fold,\n",
    "                        'perturbation_percentage': round(self.perturbation_step_size * step * 100),\n",
    "                        'accuracy': accuracy\n",
    "                    })\n",
    "                    \n",
    "                    # Accumulate confusion matrix\n",
    "                    cm = confusion_matrix(y_test, y_pred, labels=np.unique(y))\n",
    "                    if all_confusion_matrices[classifier_name][step] is None:\n",
    "                        all_confusion_matrices[classifier_name][step] = cm\n",
    "                    else:\n",
    "                        all_confusion_matrices[classifier_name][step] += cm\n",
    "            \n",
    "            # Store perturbation maps for this fold\n",
    "            perturbation_maps_per_fold.append(fold_perturbation_maps)\n",
    "        \n",
    "        # Process results for each classifier\n",
    "        processed_results = {}\n",
    "        processed_confusion_matrices = {}\n",
    "        \n",
    "        for classifier_name in self.classifiers_dict.keys():\n",
    "            # Process accuracy results\n",
    "            results_df = pd.DataFrame(all_results[classifier_name])\n",
    "            results_pivot = results_df.pivot_table(\n",
    "                index='perturbation_percentage', \n",
    "                columns='fold', \n",
    "                values='accuracy'\n",
    "            )\n",
    "            results_pivot['average'] = results_pivot.mean(axis=1)\n",
    "            processed_results[classifier_name] = results_pivot\n",
    "            \n",
    "            # Process confusion matrices\n",
    "            processed_confusion_matrices[classifier_name] = self._process_confusion_matrices(\n",
    "                all_confusion_matrices[classifier_name], classifier_name, y\n",
    "            )\n",
    "        \n",
    "        # Process perturbation maps (shared across all classifiers)\n",
    "        perturbation_maps_df = self._process_perturbation_maps(perturbation_maps_per_fold, y_train_folds)\n",
    "        \n",
    "        return processed_results, processed_confusion_matrices, perturbation_maps_df\n",
    "    \n",
    "    def test_all_classifiers(self, X, y, perturbation_steps=10, results_folder=None):\n",
    "        \"\"\"\n",
    "        Test robustness of all classifiers in the dictionary.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: feature data\n",
    "        - y: labels  \n",
    "        - perturbation_steps: maximum perturbation level to test\n",
    "        - results_folder: folder to save results CSV files\n",
    "\n",
    "        Returns:\n",
    "        - Dictionary with results for each classifier\n",
    "        \"\"\"\n",
    "        print(f\"\\nTesting robustness of all classifiers...\")\n",
    "        \n",
    "        results, cm, pm = self.test_robustness(X, y, perturbation_steps)\n",
    "        \n",
    "        all_results = {}\n",
    "        for classifier_name in self.classifiers_dict.keys():\n",
    "            all_results[classifier_name] = {\n",
    "                'results': results[classifier_name], \n",
    "                'confusion_matrices': cm[classifier_name],\n",
    "                'perturbation_maps': pm  # Shared across all classifiers\n",
    "            }\n",
    "            \n",
    "            if results_folder is not None:\n",
    "                os.makedirs(results_folder, exist_ok=True)\n",
    "                results[classifier_name].to_csv(f\"{results_folder}/{classifier_name}_robustness_results.csv\")\n",
    "                cm[classifier_name].to_csv(f\"{results_folder}/{classifier_name}_confusion_matrices.csv\")\n",
    "        \n",
    "        # Save perturbation maps only once (shared across all classifiers)\n",
    "        if results_folder is not None:\n",
    "            pm.to_csv(f\"{results_folder}/perturbation_maps.csv\")\n",
    "        \n",
    "        return all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1197b3",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bf17fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_robustness_results(classifiers_dict, results_folder, save_filename=None):\n",
    "    \"\"\"\n",
    "    Load robustness results and create a visualization plot.\n",
    "    \n",
    "    Parameters:\n",
    "    - classifiers_dict: dictionary of classifiers used in the experiment\n",
    "    - results_folder: path to folder containing the results CSV files\n",
    "    - save_filename: optional filename to save the plot (with .svg extension)\n",
    "    \"\"\"\n",
    "    # Load all robustness results CSV files\n",
    "    files = glob.glob(f'{results_folder}/*_robustness_results.csv')\n",
    "    \n",
    "    if not files:\n",
    "        print(f\"No robustness results files found in {results_folder}\")\n",
    "        return\n",
    "    \n",
    "    # Initialize a dictionary to store data\n",
    "    data = {}\n",
    "    x_labels = None\n",
    "    \n",
    "    # Read each file and store the average column\n",
    "    for file in files:\n",
    "        # Extract classifier name from filename\n",
    "        filename = os.path.basename(file)\n",
    "        classifier_name = filename.replace('_robustness_results.csv', '')\n",
    "        \n",
    "        if classifier_name in classifiers_dict:\n",
    "            df = pd.read_csv(file, index_col=0)\n",
    "            data[classifier_name] = df['average']\n",
    "            if x_labels is None:\n",
    "                x_labels = df.index  # perturbation_percentage values\n",
    "    \n",
    "    if not data:\n",
    "        print(\"No valid classifier data found\")\n",
    "        return\n",
    "    \n",
    "    # Create a DataFrame from the dictionary\n",
    "    results_df = pd.DataFrame(data)\n",
    "    \n",
    "    # Define diverse styles for each classifier\n",
    "    colors = ['#00B0F0', '#12B65C', '#FFC000', '#FF0400', '#A52A2A', '#800080', '#FFA500', '#008000']\n",
    "    linestyles = ['-', '--', '-.', ':', '-', '--', '-.', ':']\n",
    "    markers = ['o', 's', '^', 'v', 'D', 'X', '*', 'P']\n",
    "    \n",
    "    markersize = 8\n",
    "    linewidth = 2.5\n",
    "    \n",
    "    # Create style dictionary for each classifier\n",
    "    styles = {}\n",
    "    for i, classifier in enumerate(data.keys()):\n",
    "        styles[classifier] = {\n",
    "            'color': colors[i % len(colors)],\n",
    "            'linestyle': linestyles[i % len(linestyles)],\n",
    "            'marker': markers[i % len(markers)],\n",
    "            'markersize': markersize,\n",
    "            'linewidth': linewidth\n",
    "        }\n",
    "    \n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    for classifier, style in styles.items():\n",
    "        ax.plot(x_labels, results_df[classifier], label=classifier, **style)\n",
    "    \n",
    "    plt.title('Classifier Robustness to Label Perturbation')\n",
    "    plt.xlabel('Perturbation Percentage (%)')\n",
    "    plt.ylabel('Average Accuracy')\n",
    "    plt.ylim(-0.1, 1.1)\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "    plt.legend(title='Classifier')\n",
    "    \n",
    "    # Save the plot if filename provided\n",
    "    if save_filename:\n",
    "        plt.savefig(f'{results_folder}/{save_filename}')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbdce32",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7df18e",
   "metadata": {},
   "source": [
    "### Tennessee-Eastman Process (TEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f94d7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_dataset_from_folder(\"data/tep\", MAX_DATA_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2dd2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturber = CAELabelPerturber(random_state=RANDOM_SEED, epochs=NUM_EPOCHS)\n",
    "robustness_tester = RobustnessTester(\n",
    "    classifiers_dict=CLASSIFIERS,\n",
    "    perturber=perturber,\n",
    "    n_splits=N_SPLITS,\n",
    "    n_repeats=N_REPEATS,\n",
    "    perturbation_step_size=PERTURBATION_STEP_SIZE,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "_ = robustness_tester.test_all_classifiers(X, y, perturbation_steps=PERTURBATION_STEPS, results_folder=SAVE_DIR_TEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0915df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturber = CAELabelPerturber(random_state=RANDOM_SEED, epochs=NUM_EPOCHS)\n",
    "robustness_tester = RobustnessTester(\n",
    "    classifiers_dict=CLASSIFIERS_EAC_KNN,\n",
    "    perturber=perturber,\n",
    "    n_splits=N_SPLITS,\n",
    "    n_repeats=N_REPEATS,\n",
    "    perturbation_step_size=PERTURBATION_STEP_SIZE,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "_ = robustness_tester.test_all_classifiers(X, y, perturbation_steps=PERTURBATION_STEPS, results_folder=SAVE_DIR_TEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db02188e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_robustness_results(CLASSIFIERS, SAVE_DIR_TEP, save_filename='tep_robustness_plot.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6d7a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_robustness_results(CLASSIFIERS_EAC_KNN, SAVE_DIR_TEP, save_filename='tep_robustness_plot_eac_knn.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c9088b",
   "metadata": {},
   "source": [
    "### Fluidized Catalytic Cracking (FCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7810c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_dataset_from_folder(\"data/fcc\", MAX_DATA_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb86c58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturber = CAELabelPerturber(random_state=RANDOM_SEED, epochs=NUM_EPOCHS)\n",
    "robustness_tester = RobustnessTester(\n",
    "    classifiers_dict=CLASSIFIERS,\n",
    "    perturber=perturber,\n",
    "    n_splits=N_SPLITS,\n",
    "    n_repeats=N_REPEATS,\n",
    "    perturbation_step_size=PERTURBATION_STEP_SIZE,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "_ = robustness_tester.test_all_classifiers(X, y, perturbation_steps=PERTURBATION_STEPS, results_folder=SAVE_DIR_FCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c174c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturber = CAELabelPerturber(random_state=RANDOM_SEED, epochs=NUM_EPOCHS)\n",
    "robustness_tester = RobustnessTester(\n",
    "    classifiers_dict=CLASSIFIERS_EAC_KNN,\n",
    "    perturber=perturber,\n",
    "    n_splits=N_SPLITS,\n",
    "    n_repeats=N_REPEATS,\n",
    "    perturbation_step_size=PERTURBATION_STEP_SIZE,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "_ = robustness_tester.test_all_classifiers(X, y, perturbation_steps=PERTURBATION_STEPS, results_folder=SAVE_DIR_FCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45eb27ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_robustness_results(CLASSIFIERS, SAVE_DIR_FCC, save_filename='fcc_robustness_plot.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15057cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_robustness_results(CLASSIFIERS_EAC_KNN, SAVE_DIR_FCC, save_filename='fcc_robustness_plot_eac_knn.svg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
